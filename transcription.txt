 Hello all my name is Krish Nakh and welcome to my YouTube channel. So guys, yet another amazing video on Gen. TVI where I will be specifically discussing about Lama 2. Lama 2 is an open source model. Again, it has been created by Facebook or Meta and you can use this specific model even for commercial purpose. So this is quite amazing. This is an open source LLM model altogether. I will try to show you how we can use this, create an end to end project also in this specific video. So there are many things that are going to happen and probably whatever topics that I teach going forward that is related to Gen. TVI. I will definitely follow this kind of approach so that you also get a brief idea about all these kind of models. So what is the agenda of this particular video? The agenda is that we will get to know about Lama 2. Then we will go ahead and see the research paper where I will be talking about the key points about the Lama 2 model. Again, since this is an open source and soon Lama 3 is also going to come up. So that is the reason I am going to create this particular video. I really want to be in sync with all the open source LLM models that are coming up. Then we will go and apply and download the Lama 2 model. So we will be seeing like how we can actually use this particular model in our project also. So for that purpose, I will be downloading this model. You have to also apply this in the meta website itself. And there is also one way how we can also use it directly from hugging face. So I will also show you that. And after that, we will try to create an end to end LLM project and this will be a blog generation LLM app. All the topics I will be covering it. I know it will be a little longer video but every week one kind of this kind of video is necessary for you all. And since 2024, I have the target. I really need to teach you generative way in a way that you can understand it and use it in your industries also. So I will keep a target. So every video I will keep a target like this. Target for this particular video is 1000 likes not 1000 likes but 1000 likes and comments. Please make sure that you write some comments and I'll keep the target 200. Okay. This will actually motivate me. This will probably help this particular video to reach to many people through which they can actually use this. And entirely this is completely free which will also be beneficial for you. And my aim is to basically democratize the entire AI education. Okay. So let's go ahead and let's first of all start with the first one that is introducing LAMA to what exactly is RAMA to LAMA to is then again open source a large language model. It can be it is used and it is available for free for research and commercial purpose. You can actually use this in your companies in a startup wherever you want to use it. Okay. Now let's go ahead and read more about it. So inside this model, it has till now LAMA to has released three different model size. One is with 7 billion parameters. The other one is 13 billion parameters and the best one is somewhere on 70 billion parameters. Three training tokens is taken somewhere on two trillion context length is 4096. Again, when I say that if I probably compare most of the open source models, I think LAMA 2 is probably very good. We'll be seeing all those metrics also. So here you can see LAMA 2 is pre trained models are trained on two trillion tokens and have double the context length and LAMA will for one. It's fine tune models have been trained on over one million human annotation. Okay. Now let's go ahead and see the benchmark and this is with respect to the benchmarking with all the open source models. So it is not comparing with chat GPT sorry GPT 3.5 GPT 4.0 or Palm 2. So all the open source models here you can probably see this is the three version 7 billion, 13 billion, 65 billion, 70 billion, right? All LAMA 2. LAMA 1 was 65 billion. One model it had over there. So if you see LAMA 2 with respect to all the metrics is very good. MMLU that is with respect to human level understanding Q&A. All the performance metrics is superb natural language processing GSM8K human Eval. In human Eval it is probably having a less when compared to the other open source models. So here you can see in human Eval human Eval human Eval basically means with respect to writing code, code generation there it has a lot of problems. So here you can see 12.8 18.3 it is less. It is less when compared to all the other open source models over here. And there are some other parameters you can probably see over here with respect to different task you can see the performance metrics. So this was more about the model now let us go ahead and probably and this is one very important statement that they have come up with. We support an open innovation approach to AI responsible and open innovation give us all a stake in the AI development process. So yes Facebook is again doing a very good work and then soon they are also going to come up with the Lama 3 model. Now let us go ahead and see the research paper. So here is the research paper the entire research paper. Now see what you should really focus on a research paper you know in research paper they will be talking about how they have actually trained the model what kind of data points they have they actually taken in order to train the model and all right. So over here you can see that in this work we developed and released Lama 2 a collection of pre-trained and fine tuned large language models ranging in scale from 7 billion to 70 billion parameters. So if we talk about parameters it is somewhere on 7 billion to 70 billion. Our fine tuned LLMs called Lama 2 chat are optimized for dialogue use cases just like a chat bot and all right. More information you can probably see over here what is the pre-training data see. So they have told that our pre-training data includes a new mix of data from publicly available sources which does not include data from meta products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. Now this is where ethics comes into picture they really want to use this AI in a responsible way right. So we trained on 2 trillion tokens and obviously for all these things you have to use NVIDIA GPU okay. I know guys it is boring to read the research paper but it is good to have all the specific knowledge. So please keep your energy up was this video till the end then only you will be able to understand things right. Not only here because later on you will be having other models like Mistral I will probably create a video on Mistral also in the upcoming video right. So everywhere with an end to end project everything I will take this format let me know whether you are liking this format on up. So training data we adopt most of the pre-training settings and model architecture from Lama 1. We use the standard transformer architecture now you can understand how important transformer is right. Most of the open source model are based on transformer architecture itself right. We trained using Adam W. Optimizer okay with so and so parameters we use consign learning rate schedule with so and so and here you could probably see with respect to the performance like how well it was training PPL process tokens how many tokens was actually done with respect to all the different varieties of Lama model. Now this is basically the training loss you can probably see training loss for Lama 2 okay. This is also important training hardware and carbon footprint it is basically saying that how much it is using they use Nvidia a 100 I have seen this GPU it is quite amazing it is very huge okay and it is very fast also but again with such a huge amount of data it is also going to take time right. So all these things are there you can also see time how much time it has basically taken how many hours 70 billion this many number of hours power consumption this this all information is there right. This is good to have right all you should know like we just taking more energy and all right and here with respect to the Lama 2 you can probably see with respect to common reasoning it is very good when compared to all the other models open source model world knowledge reading comprehension math MMLU math it is very little bit less you can see over here when compared to other model I think it is still 35 itself but remaining all it has basically come I think this 35 is also greater than all these things right. MMLU is very much good it is able to achieve till 68.9 Google Gemini I said that this is to 90 percent okay but again this is the thing that you really need to know some more information fine tuning fine tuning also okay this is very much important guys it has it has used this reinforcement learning okay and with human feedback so R L HF basically means reinforcement learning with human feedback and this is what chat deputies also trained with right. So definitely I think as we go ahead as we go ahead and see Lama 3 and all it is going to give us very good accuracy I guess okay. So supervised fine tuning if you go and just check how generative AI how LLM models are trained you will be able to get a video on this I created a dedicated video where I have explained about supervised fine tuning how does supervised fine tuning happen what how does R H L F happens right reinforcement sorry R L HF human feedback happens all those things I have actually explained. So here you can see some of the prompts right up to help me remember the first 10 elements on the periodic table hydrogium come first as the element 1 William is second for balloons this this I want you to roast me now see this statement is also very important right. So I want you to roast me I want you to make it particular brooders swearing at me so what is saying I am sorry but I cannot comply with that request using vulgar language or intentional hurting someone feelings is never expectable. So some kind of feelings they are trying to bring inside all this kind of models okay except the annotation is basically there you can probably read all this things this is good to have good to learn how this reinforcement learning with human feedback was done and all everything is given over here. So this was all about the research paper still there are many papers to go ahead you can probably go ahead and check it out there is a concept of reward modeling also reward is also given right the parameters they have used to separate parameters over here and various kind of test is basically done. So this was all the information about this now the next thing is that how you can go ahead and apply or download this specific model just click on download the model over here. So the third part provide all the information over here and what all things you specifically required like llama to and lama chat code llama lama guard. So go ahead and just put all this information and click on submit after submitting probably little take 30 minutes and you will start getting this mail okay you all start to start building with code llama you will also be getting the access from llama to see you will be getting this entirely right model weights available all the models weight will be given to you in this specific link you can click and download it also if you want so that you can use it in your local or you can deploy it wherever you want okay. So this kind of mail you will be getting llama to commercial license all the information with all the info over here and this all models it is specifically giving again I told you 70 B 70 B chat why this two models are there this is specifically for Q and a kind of application dialogue flow application I can basically say remaining one can be used for any kind of task in a complex scenario and all okay. So once you do this the next thing is that you can also go to hugging face in hugging face you have this llama to 70 B chat FF and there is the entire information that is probably given about the entire model itself you can probably read it from here with respect to this llama to is a collection of pre-trained this information is basically there you can also directly use it if you want the code with respect to transformer just click on use and transformer you will be able to get this entire code where you can directly use this also okay. What we are basically going to do I am not going to use 70 billion parameters since I am just doing it in my local machine with the CPU itself okay. So what I will do I will be using a model which is basically it is basically a quantized model right with respect to this same llama model it is called as llama to 70 B chat GGML. So if you go ahead and see this you will be able to see that this particular model you will be able to download it and you will be able to do use it it is just like a good version but less parameter versions right. So when we say quantize that basically means this model has been compressed and probably provided you in the form of weight. So what you can do any of this model so recent model what you can do which is of 7.16 GB you will first of all download it so I have already downloaded it so I am just going to cancel it over here okay because I have already downloaded it over here okay so I will do that specific download over here and then you can probably go ahead and start working on this and start using this and now how you can probably use it I will show you by creating an end to end project. So for creating an end to end project what are the steps again the project name that I have already told is basically a block generation LLM app here I am going to specifically use this open source llama to model again I am going to use the hugging face API also for that and let us see how the specific step by step how we will be doing this specific project. So let us go ahead and let us start this particular project okay guys now let us start our block generation LLM platform application so the model that I had actually generated over here you can probably see the model over here in the bin size and this is the size of the model is over here I am going to specifically use in my local machine for my local inferencing and all so over here what I will do I will go quickly go ahead and open my VS code so my VS code is ready over here okay now let us go ahead and do step by step things that we really need to do first of all I am just going to create my requirement dot txt file requirement dot txt file and now I will go ahead and open my terminal so I will go ahead and open my command prompt and start my project okay so quickly I will clear the screen I will deactivate the default environment condo deactivate okay and we will do it step by step so first step as usual go ahead and create my environment condo create minus p vnv environment I hope I have repeated this specific step lot many time so here I am going to create condo create minus p vnv with python w equal to 3.9 y okay so just to give you an idea what how exactly it is going to run how things are basically going to happen step by step will understand so first of all we are creating the environment and then we will go ahead and fill our requirement dot txt now in requirement dot txt I am going to specifically use some of the libraries like sentence transformers see transformer fast API if you want to specifically use fast API I will remove this fast API I think I will not require this IPI kernel so that I can play with Jupyter notebook if I want I can also remove this I do not want it Langchon I will specifically using and stream let I will be using okay so first of all I will go ahead and create condo activate okay condo activate vnv so we have activated the environment and the next thing is that I will go ahead and install all the requirement dot txt okay and in this you do not require okay okay I have not saved it so requirement dot txt is not saved now in this you do not require any open AI key because I am just going to use hugging face and from hugging face I am going to probably call my model which is basically presented my local so here is the model that I am going to specifically call okay so once this installation will take place then we will go ahead and create my app.py and just give you an idea like I am going to basically create the entire application in this specific file itself so quickly let's go ahead and import our stream let so till the installation is basically happening I will go ahead and install stream let okay as t and then along with this I will also be installing Langchon dot prompts because I am also going to use prompts over here just to give you an idea how things are going to happen it is going to be very much fun guys because open source right it is going to be really amazing with respect to open source you do not require anything as such and then I am going to basically write prompt template because we need to use this from Langchon then I will be also using from Langchon Langchon dot lm I am going to import C transformer okay why this is used I will just let you know once I probably write the code for this okay so tree tree transformer also I am going to basically use over here so this is going to be from okay so C transformers prompt template and ST for the stream let I am going to specifically use the first thing is that I will go ahead and write function to get response from my LLLama model right Lama 2 model I am going to basically use this okay still the installation is taking place guys it is going to take time because there are so many libraries I have been installing okay so I will create a function over here let's create this particular function later on okay now after this what I am actually going to do is that if you will go ahead and set our stream let right set underscore page underscore config see now many people will say stream let or flask it does not matter guys anything you can specifically use stream let why I am specifically using is that it will be very much easy for me to probably create all this things right the UI that I want so in set page config I am going to basically use page title generate blocks page icon I have taken this robot icon from the stream let documentation layout will be central and initial sidebar will be collapsed okay so I am not going to open the sidebar in that specific page now I will keep my ht.header so ht.header in here I am going to basically generate my blocks right so generate the blocks and I will use the same logo if I want right looks good okay so this is the next thing I will probably this will be when I head over here first of all I will create my input text okay so input text field right and this will basically be my input text field and let me keep it as a text area or a text box whatever things is required so I will write go ahead and write ht.ht.input text underscore input okay so this will basically be my ht so let's see everything is working fine why this is not coming in the color okay still the installation may be happening so over here I will go ahead and write this I will say enter the blog topic right so if you just write the blog topic it will should be able to give you the entire blog itself with respect to anything that you want okay so done the installation is basically done over here you can probably see this good I will close this up now I will continue my writing the code so I have created my input box now the other thing that I really want to create is that I will try to create two more columns or two more fields below this box okay one field I will say that how many words you specifically want for that blog okay so over here creating two more columns for additional two fields additional two fields okay so here first of all will be my column one let's say column one and column two I will just write it like this and here I will say ht.columns and here I will be using I will be giving right what should probably be the width like let's say 5.5 I'm giving you'll be able to see that the width of the text box or width of the column that I specifically have I'll be able to see it okay I'm just creating that width for that columns okay now I'll say width column one whenever I probably write anything in the column one or select in anything in the column one this will be basically be my number of words okay number of words and for here I will be creating my st.text input and this text input will probably retrieve the details of a number of words okay so here I have specifically a number of words great now the next column that I specifically want a detail so for whom I am actually creating this particular blog I want to probably put that field also so with column three I will probably create something like this I will say okay fine what blog style I will create a field which is basically called as blog style okay now inside this blog style what I am actually going to do sorry not column three column two because I've created those variables over there okay so the blog style will basically be a drop down so I will say st.selectbox okay and I will say what box this is specifically for so that first message I will say select write writing the blog for for okay so this I'm basically going to say that okay for whom I'm going to write this particular blog okay and with respect to this I can give all the options that I really want to give okay so for giving the options I will also be using this field so let's say the first option will be for researchers whether I'm writing that particular blog for researchers or for data scientist okay data scientist or I am basically writing this blog for for common people okay common people so this three information I really want over here and this will basically help me to put some styling in my blog okay that is the reason why I'm basically giving over here okay and by default since we need to select it in the first option so I will keep it as index as zero okay so here is all my styling that I have specifically used so if you want to probably make it in this way so you'll be able to understand this so this will be my column one and this is basically be my column two okay and then finally we will go ahead and write submit button submit will be st.button and this will basically be my generate okay so I'm going to basically generate this entirely generate just like a button which will basically click by taking all this particular information so from here I'll be getting my input from here I'll be getting number of words from here I'll be getting my blog style okay all these three information now this will actually help me to get the final response here okay so I will say if submit okay if submit I have to call one function right and what will be that specific function that function will return me some output okay and that output will be displayed over here now that function I really need to create it over here itself let's say I will say get Lama response okay so this is basically my function and this I will create in my definition and what are parameters I specifically require over here right the this three parameters right and if I probably call this function over here what are the parameters that I'm going to write over here is all this three parameters so first parameter is specifically my text input input text the second parameter that I'm actually going to give over here is number of words the third parameter that I really want to give is my blog style so like what blog style I really want okay so all this three information is over here so this will basically be my input text okay I'll write the same name no worries number of words and third parameter is basically my block so all these materials will be given in the description if you are liking this video please make sure that you hit subscribe press the bell notification I can hit like again just to motivate me okay if you motivate me a lot I will create multiple contents amazing content for you okay now here is what I will be calling my Lama model right Lama model Lama 2 model which I have actually downloaded in my local and for that only I will be specifically using this C transformers right now if I probably go ahead and search in Lang Chen Lang Chen see whenever you have any problems related to anything as such right C transformers C transformers go and search in the documentation everything will be given to you so C transformers what exactly it is it is it is over here it is given over here or not here let's see the documentation perfect so here you can see C transformers the C travel library provides Python binding for GGM models so GGM models the blog GGM models whichever model is basically created you can directly call it from here let's say in the next class I want to call Mistral so I can go ahead and write my model name over here as Mistral and to be able to call directly from the hugging phase okay not only hugging phase but at least in the local if you have the local if you want to call it from the hugging phase then you have to probably use the hugging phase API key but right now I don't want to use all those things so I want to make it quite simple so CC transformers and here I am going to basically write my model model is equal to and this should be my model path right which model path this one model slash this one right so here you can probably see this specific name v3 q8 0 mean okay so I am going to probably copy this entire path and paste it over here okay so this will basically be my model and inside this what kind of model type I want there is also a parameter which is basically called as model type and in model type I am going to basically say it is my lama model okay and you can also provide some config parameter if you want otherwise it will take the default one so I will say max new underscore tokens is equal to 256 and then the next one will basically be my temperature colon 0.01 let me keep the temperature point less only so I want to see different different answers okay so this is done this is my LLM model that I am basically going to call from here and it is going to load it okay now after my LLM model is created I will go ahead and write my prompt template because I have taken three three different information so template here I will go ahead and create this will be in three quotes if you want to write it down because it is a multi line statement and I will say write a blog for which style right blog style for whom for this specific blog style for researchers for freshers for anyone you can write or I will say job profiles as I can for research a job profile for fresher job profile for normal people job profile something like this job profile for a topic which topic I am going to basically say this will be my number of words sorry not number of words this will be my input underscore text so this is how we basically write prompts within how many words the number of words okay this many number of words I am going to basically write this okay so this actually becomes my prompt template entirely okay this is my entire prompt template write a blog for so and so for blog style this this this to make it look better what I will do I will just press tab so that it will look over here okay so this is my template that I am probably going hard with I have given the three information block style input text number of words everything is given over here now finally I need to probably create the prompt template okay so for creating the prompt template I am going to use prompt is equal to prompt template and here I am going to basically give my input variables so input underscore variables and inside this I am going to basically write first information that I want what kind of input size specifically want right whether I want this block style so for block style I can just write style second one I can probably say text third one I can basically say and underscore words so this will basically be my three information that I am going to provide it when I am given in my prompt template okay and finally this is my input variables this next parameter that I can also go ahead with I can provide my template itself what template I want to give so this is be my template over here now finally we will generate the response from the lama model okay lama two model which is from ggml okay so here what I am actually going to do I am going to basically write lm and whatever things we have learned in length till now prompt dot prompt dot format and here I am going to basically use email sorry email what is the information that I really want to give over here prompt dot format so the first thing is with respect to style the style will be given as block style so I am going to basically write blog underscore style okay the next information that I am probably going to give is my input text input text is equal to not input text text is equal to input text I have to give text is equal to input underscore text and the third parameter that I am going to give is my n underscore words which will basically be by number of words done so this is what I am specifically giving with respect to my prompt and what lm will do it will try to give you the response for this and then we will go ahead and print this response and we will return this response also okay response response response okay and what will do we will go ahead and return this response so step by step everything is done now I am going to call this get lama response over here already is done now let's see if everything runs fine or not hope so at least one error will at least come let's see so I will delete this and let's go ahead and write away to run the stream let app all you have to do is just just write stream let run app dot p1 okay so once I probably execute this you'll be able to see this is what is my model but still I am getting a module stream let has attribute no head okay so let's see where I have specifically done the mistake because I think it should not be head it should be header okay I could see the header header okay fine no worries let's run it baby let's run this again stream let run app dot p1 I think it should run this looks good enter the block topping number of words researcher writing the researcher block data scientist common people so let's go ahead and write about large language model so 300 words so number of words I will go ahead and write 300 I want to basically write it for common people and we will go ahead and generate it now see as soon as we click on generate it is going to take some time the reason it is probably going to take some time because we are using this particular in my local CPU but we got an error let's see key error block style it seems so I will go to my code block style block style block so one minor mistake that I have specifically done over here so what I will do is that I will give the same key name so that it does not give us any issue okay so this will be my input text number of words the thing is that whatever things I give in the prompt template the input variables should be of that same name okay so that is a mistake I have done it's okay no worries so let's go ahead and execute it now everything looks fine have him assign the same value with their number of words number of words so here also I will go ahead and write number of words block style input text and this also should be block style the name I am giving same right for both prompt template and this okay so I think now it should work let's see so go ahead and write this and now my page is opened now I will go ahead and write large language models and it will probably create my words so this will be 300 I want to create it for how many people that's generated as I said that the output that I'm probably going to get is going to take some time because I'm running this in local CPU let's say if you deploy this in the cloud with respect to if there are GPU futures then you will get the response very much quickly so just let's wait till then we get the output hardly but I think it is 5 to 10 seconds max and since I've told 300 words it is again going to take time so let's see the output once it comes so guys it hardly took 15 seconds to display the results so here you can see that large language models have become increasingly popular and recent here due to the in due to the ability to process and generate human like languages it looks like a good blog you can also created any number of words blog itself now understand that I have a good amount of RAM my CPU has lot of cores so I was able to get it in 15 seconds for some of the people it may take 20 seconds it may third take 30 seconds now you may be asking krish how can you specifically reduce this time it is very much simple guys we will probably do the deployment in AWS or any other cloud server itself which I will be probably showing you in the upcoming videos and there you'll be able to see that how with the help of GPUs the inferencing also becomes very much easy not only that we'll also see how we can probably fine tune all this data set with your own custom data itself right because these all are very big big models and I'm just taking the 7 GB model over here this is with respect to 7 billion parameters right but still it is able to give you a very good data itself right so I hope you like this particular video this was it for my side if you like this particular video please make sure you hit like share and share with all your friends I'll see you all in the next video have a great day thank you one doll take care bye bye